{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd0599-9237-4272-a351-836404c662d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DataFrame Extraction part 3. how to extract data from the pyspark dataframe to a hdfs local rdbms or hive tavle or to any\\\n",
    "file format \\ like csc,parquet etc')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1739471-0dd3-480d-82a9-23afe468912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.groupby(df.col2).agg(count(df.tid)).show()\n",
    "# select count(tid),col2\n",
    "# from df\n",
    "# groupby col2\n",
    "# this means all the conditions to be substituted in select clause will work as agg function\n",
    "\n",
    "\n",
    "# select count(id)\n",
    "# from table where group by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5bcabadd-618e-4d35-9c2a-a024c2e35cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=11599, status='CLOSED'), Row(id=2, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=256, status='PENDING_PAYMENT'), Row(id=3, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=12111, status='COMPLETE')]\n"
     ]
    }
   ],
   "source": [
    "# Csv file\n",
    "df=spark.read.csv(path='RetailDB+SalesData/RetailDB SalesData/Orders/part-00000' ,schema='id int,col2 timestamp, tid int,status string')\n",
    "print(df.take(3))\n",
    "# print(df.show())\n",
    "# df.filter((df.tid<1111) & (df.status=='PENDING_PAYMENT')).groupBy(df.id).agg(count(df.id),count(df.tid),count(df.status)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f88bbc5d-f5a6-4566-87c6-58fa220a2dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neosoft/Desktop/pyspark learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(df.write.csv)\n",
    "# Extract tp Csv\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "\"\"\"df.write.save(\n",
    "    path: Union[str, NoneType] = None,\n",
    "    format: Union[str, NoneType] = None,\n",
    "    mode: Union[str, NoneType] = None,\n",
    "    partitionBy: Union[str, List[str], NoneType] = None,\n",
    "    **options: 'OptionalPrimitiveType',\n",
    ") -> None\"\"\"\n",
    "\n",
    "\n",
    "df_count=df.groupby(df.status).count()\n",
    "\n",
    "df_count=df_count.coalesce(2)\n",
    "# df_count.show()\n",
    "df_count.rdd.getNumPartitions()\n",
    "# df_count.write.save(path='file_save/new',mode='overwrite',format='csv',sep='@',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d324a82e-b549-42cb-bbfa-511f104d0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc0785b4-6025-4bec-9a6f-da689baeaf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions Before Coalesce: 4\n",
      "Number of Partitions After Coalesce: 2\n"
     ]
    }
   ],
   "source": [
    "# Assume 'df' is your DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22), (\"David\", 35)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the number of partitions before coalesce\n",
    "print(\"Number of Partitions Before Coalesce:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Coalesce the DataFrame to reduce the number of partitions to 2\n",
    "coalesced_df = df.coalesce(2)\n",
    "\n",
    "# Show the number of partitions after coalesce\n",
    "print(\"Number of Partitions After Coalesce:\", coalesced_df.rdd.getNumPartitions())\n",
    "\n",
    "# Stop the Spark session\n",
    "\n",
    "df.coalesce(2).write.save(path='file_save/new'# Extract tp Csv,mode='overwrite',format='csv',sep='@',header=True)\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41e8fe-c74c-44d1-9430-b3e2590b89b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9fc2a140-83ec-4d8c-909e-8f8843afcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  2|Alice|\n",
      "|  5|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cast a datatype\n",
    "\n",
    "\n",
    "# help(df.age.cast)\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "[(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
    "\n",
    "df=df.withColumn('age',df['age'].cast(\"string\"))\n",
    "df.show()\n",
    "# print(df.dtypes)\n",
    "# df=df.select(df.name,df.age.cast('string'))\n",
    "# df.printSchema()\n",
    "# df.select(df.age.cast(\"string\")).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "031d4095-9cd0-4597-ba75-ec55eb64e6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = false)\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Row(id='0'), Row(id='1')],\n",
       " [Row(id='2'), Row(id='3'), Row(id='4')],\n",
       " [Row(id='5'), Row(id='6')],\n",
       " [Row(id='7'), Row(id='8'), Row(id='9')]]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract to text\n",
    "\n",
    "df=spark.range(10)\n",
    "df=df.withColumn('id',df.id.cast('string'))\n",
    "# df\n",
    "df.printSchema()\n",
    "df.show()\n",
    "# df.write.save(format='text',path='file_save/text')\n",
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "07c709c2-ad8f-491e-8967-96f863a8efed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=11599, status='CLOSED'), Row(id=2, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=256, status='PENDING_PAYMENT'), Row(id=3, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=12111, status='COMPLETE')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract to parquet\n",
    "df=spark.read.csv(path='RetailDB+SalesData/RetailDB SalesData/Orders/part-00000' ,schema='id int,col2 timestamp, tid int,status string')\n",
    "print(df.take(3))\n",
    "\n",
    "# help(df.write.parquet)\n",
    "\n",
    "# df.write.save(format='parquet',path='file_save/parquet',mode='overwrite')\n",
    "df.write.save(format='parquet',path='file_save/parquet/partby',partitionBy='status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "41aa89d9-f8e3-4f99-80aa-4a780c9e0147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=11599, status='CLOSED'), Row(id=2, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=256, status='PENDING_PAYMENT'), Row(id=3, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=12111, status='COMPLETE')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract to orc\n",
    "df=spark.read.csv(path='RetailDB+SalesData/RetailDB SalesData/Orders/part-00000' ,schema='id int,col2 timestamp, tid int,status string')\n",
    "print(df.take(3))\n",
    "\n",
    "# help(df.write.parquet)\n",
    "\n",
    "df.write.save(format='orc',path='file_save/orc',mode='overwrite')\n",
    "# df.write.save(format='parquet',path='file_save/parquet/partby',partitionBy='status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a755821-a234-432e-ac7f-4fe93efbf522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "17468567-f973-448e-9cad-6e20939f4f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=11599, status='CLOSED'), Row(id=2, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=256, status='PENDING_PAYMENT'), Row(id=3, col2=datetime.datetime(2013, 7, 25, 0, 0), tid=12111, status='COMPLETE')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract to json\n",
    "df=spark.read.csv(path='RetailDB+SalesData/RetailDB SalesData/Orders/part-00000' ,schema='id int,col2 timestamp, tid int,status string')\n",
    "print(df.take(3))\n",
    "\n",
    "# help(df.write.parquet)\n",
    "# df.rdd.getNumPartitions()\n",
    "df.write.save(format='json',path='file_save/json',mode='overwrite',compression='bzip2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a377527-7b52-469b-81a5-687457a38c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7d842-4798-4d33-a7b5-2d3644190acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract to avro(row based-write intensive)\n",
    "# avro is third party package so install avro with pip install avro them use it below code will not work\n",
    "df=spark.read.csv(path='RetailDB+SalesData/RetailDB SalesData/Orders/part-00000' ,schema='id int,col2 timestamp, tid int,status string')\n",
    "print(df.take(3))\n",
    "\n",
    "# help(df.write.parquet)\n",
    "# df.rdd.getNumPartitions()\n",
    "df.write.save(format='avro',path='file_save/avro',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2083c1c3-472a-41f8-bcec-d8b17249e06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[215], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# help(df.write.saveAsTable)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[43mdefault\u001b[49m\u001b[38;5;241m.\u001b[39mtable)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'default' is not defined"
     ]
    }
   ],
   "source": [
    "# help(df.write.saveAsTable)\n",
    "# hive and jdbc table remaining while loading the pyspark register the jar file or package of jdbc to use it with pyspark.\n",
    "df.write.saveAsTable(default.table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
