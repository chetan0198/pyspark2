{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8d2f37-7119-4707-a088-f6f45cbcc988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files is udf\n"
     ]
    }
   ],
   "source": [
    "print('reading files is udf. it is an interface used to load different types of files in pyspark or load df from external storage systems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bd8ef05-5fff-4bfe-b869-b72094883ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " types of files in spark are csv, text, orc parquet, json, hive table, jdbc table, avro\n",
      "if we dont give any schma it will consider all columns are considerred as string if we give inferchema as true it will try to inferschema from the data. sep is used to specidy separator.\n"
     ]
    }
   ],
   "source": [
    "print(' types of files in spark are csv, text, orc parquet, json, hive table, jdbc table, avro')\n",
    "print('if we dont give any schma it will consider all columns are considerred as string if we give inferchema as true it will try to \\\n",
    "inferschema from the data. sep is used to specidy separator. header will make first record as the header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a62837-369b-496e-bd8c-3201579b5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-csv\n",
    "csv_file=spark.read.load(path='RetailDB+SalesData/RetailDB SalesData/Order_items/part-00000',format='csv',schema='id int,oid int, price int,quantity int,subtotal int,total int')\n",
    "csv_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19404d1-7e65-4ac8-ae29-c082611f322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-csv\n",
    "csv_file=spark.read.csv(path='RetailDB+SalesData/RetailDB SalesData/Order_items/part-00000',schema='id int,oid int, price int,quantity int,subtotal int,total int')\n",
    "csv_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f81645-4088-4d69-957e-252cb4f7249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- oid: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- subtotal: integer (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f396741-eb95-4221-9fd3-4643b73f804d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14b832c8-5a14-493e-8f51-af721699fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|value                    |\n",
      "+-------------------------+\n",
      "|1,1,957,1,299.98,299.98  |\n",
      "|2,2,1073,1,199.99,199.99 |\n",
      "|3,2,502,5,250.0,50.0     |\n",
      "|4,2,403,1,129.99,129.99  |\n",
      "|5,4,897,2,49.98,24.99    |\n",
      "|6,4,365,5,299.95,59.99   |\n",
      "|7,4,502,3,150.0,50.0     |\n",
      "|8,4,1014,4,199.92,49.98  |\n",
      "|9,5,957,1,299.98,299.98  |\n",
      "|10,5,365,5,299.95,59.99  |\n",
      "|11,5,1014,2,99.96,49.98  |\n",
      "|12,5,957,1,299.98,299.98 |\n",
      "|13,5,403,1,129.99,129.99 |\n",
      "|14,7,1073,1,199.99,199.99|\n",
      "|15,7,957,1,299.98,299.98 |\n",
      "|16,7,926,5,79.95,15.99   |\n",
      "|17,8,365,3,179.97,59.99  |\n",
      "|18,8,365,5,299.95,59.99  |\n",
      "|19,8,1014,4,199.92,49.98 |\n",
      "|20,8,502,1,50.0,50.0     |\n",
      "+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read text (contains only one column with all rows if wholetext=True onlu one row full filw)\n",
    "text_file=spark.read.load(path='RetailDB+SalesData/RetailDB SalesData/Order_items/part-00000',format='text')\n",
    "text_file.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1774f3-7155-4624-8ed1-0ca3291c3083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072435a-81ad-467d-82e6-6637dcbb9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read orc and parquet file (both are columnar file foemats and are used to the reading quickly and avro is used for writing it is row based\\\n",
    "# file format)\n",
    "\n",
    "orc_file=spark.read.load(path='files/orc-file-11-format.orc',format='orc')\n",
    "orc_file.show(1)\n",
    "orc_file.printSchema()\n",
    "\n",
    "orc_file.select(\"boolean1\", \"byte1\", \"int1\").show()\n",
    "\n",
    "orc_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb7db4-ed6d-412e-be7e-96344e2ff190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read  parquet file\n",
    "# help(spark.read.parquet)\n",
    "par_file=spark.read.load(path='files/sample1.parquet',format='parquet')\n",
    "par_file.show()\n",
    "par_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a94648-c7cb-47e7-b50d-7bc12101956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read  avro file\n",
    "# help(spark.read.parquet)\n",
    "par_file=spark.read.load(path='files/Sample-Avro-Schema',format='avro')\n",
    "par_file.show()\n",
    "par_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822d9b1-ee6a-4235-9fb3-90d4ce768d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7442e9-5f93-40d1-87e2-51c80133b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can in the samw way load hive table jdbd table with oracle sql server, redshift dw, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c784cd9-3e50-4aa6-8f71-a619c72eb6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a28d6310-eb54-46f4-bce0-ae4e8d650dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Catalog\n",
    "# help(spark.catalog.currentDatabase)\n",
    "spark.catalog.currentDatabase()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
